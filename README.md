# Build-GPT-from-Scratch
This repository is a complete implementation of a decoder-only, GPT-style Transformer language model, built from scratch in PyTorch. The project follows the outstanding "Let's build GPT" lecture by Andrej Karpathy, demonstrating a ground-up understanding of modern Large Language Model (LLM) architecture.

# Project Overview
The goal of this project is to demystify the core components of models like GPT by implementing them step-by-step. The model is trained on a character level to generate text (in this case, Shakespeare-like text from the Tiny Shakespeare dataset).

# Credit and Inspiration
This entire project is an implementation based on the public lecture by Andrej Karpathy. All credit for the code structure, teaching, and inspiration goes to him.

Video: ["Let's build GPT: from scratch, in code, spelled out."](https://www.youtube.com/watch?v=kCc8FmEb1nY) 

Instructor: Andrej Karpathy
